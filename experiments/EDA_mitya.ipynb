{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "кол-во чеков = кол-во походов в магазин"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crazy-акция - по всем группам товаров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model, metrics, preprocessing\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Dataset\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.core import Experiment, Workspace\n",
    "\n",
    "import sys\n",
    "sys.path.append('./scripts/')\n",
    "from scripts.pipeline import StupidModel, run, validate_on_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Your pandas and pyarrow versions are incompatible. Please install pyarrow>=0.12.0 for improved performance of to_pandas_dataframe. You can ensure the correct version is installed by running: pip install pyarrow>=0.12.0 --upgrade\n"
     ]
    }
   ],
   "source": [
    "# Log In to Azure ML Workspace\n",
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"76f90eb1-fb9a-4446-9875-4d323d6455ad\")\n",
    "\n",
    "# Initialise workspace\n",
    "ws = Workspace.from_config(auth=interactive_auth)\n",
    "\n",
    "# Data import \n",
    "aml_dataset = Dataset.get_by_name(ws, 'train_ds', version='latest')\n",
    "data = aml_dataset.to_pandas_dataframe()\n",
    "data.set_index('CardHolder', inplace=True)\n",
    "\n",
    "# Save it to the further opeartions\n",
    "original_columns = data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Графика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take random sample in each subgroup with same size\n",
    "sample_size = 10000\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(18, 10))\n",
    "sales_sum = ['sale_sum_3m_g{}'.format(i) for i in [24, 26, 32, 33]]\n",
    "\n",
    "plt.subplot(221)\n",
    "sample = stubborn.loc[np.random.choice(stubborn.index, sample_size, replace=False)]\n",
    "sns.scatterplot(sample[sales_sum].sum(axis=1), sample['response_sms'])\n",
    "plt.title('Участники тестовой группы, которые не пошли на акцию')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.hist(sample['response_sms'])\n",
    "\n",
    "plt.subplot(222)\n",
    "sample = active.loc[np.random.choice(active.index, sample_size, replace=False)]\n",
    "sns.scatterplot(sample[sales_sum].sum(axis=1), sample['response_sms'])\n",
    "plt.title('Участники тестовой группы, которые пошли на акцию')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.hist(sample['response_sms'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Outliers\n",
    "Первоначальное избавление от выбросов поможет в дальнейшнем грамотнее исправить ошибки в данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any group number is in diaposon from 20 to 79\n",
    "all_groups = [i for i in range(20, 80)]\n",
    "\n",
    "# This function returns columns from 'columns' list, which are met in columns of \"data\"\n",
    "def get_columns_list(data, columns):\n",
    "    return data.columns[data.columns.isin(columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем интересующие нас признаки: некоторые уже есть в датасете, некоторые придется собирать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of a feature : (columns to collect from, method)\n",
    "# methods : mean, max, min, sum\n",
    "collect_features = {\n",
    "    'cheque_count_12m_sum'             : (get_columns_list(data, ['cheque_count_12m_g{}'.format(i) for i in all_groups]), 'sum'),\n",
    "    'children'                         : (['children'], 'max'),\n",
    "    'crazy_purchases_cheque_count_12m' : (['crazy_purchases_cheque_count_12m'], 'max'),\n",
    "    'k_var_disc_share_6m_max'          : (get_columns_list(data, ['k_var_disc_share_6m_g{}'.format(i) for i in all_groups]), 'max'),\n",
    "    'k_var_sku_price_6m_max'           : (get_columns_list(data, ['k_var_sku_price_6m_g{}'.format(i) for i in all_groups]), 'max'),\n",
    "    'sale_sum_12m_sum'                  : (get_columns_list(data, ['sale_sum_6m_g{}'.format(i) for i in all_groups]), 'sum'),\n",
    "}\n",
    "\n",
    "# Add collections\n",
    "for key in collect_features.keys():\n",
    "    method = collect_features[key][1]\n",
    "    \n",
    "    if method == 'mean':\n",
    "        data.loc[:, key] = data[collect_features[key][0]].mean(axis=1)\n",
    "    elif method == 'sum':\n",
    "        data.loc[:, key] = data[collect_features[key][0]].sum(axis=1)\n",
    "    elif method == 'max':\n",
    "        data.loc[:, key] = data[collect_features[key][0]].max(axis=1)\n",
    "    elif method == 'min':\n",
    "        data.loc[:, key] = data[collect_features[key][0]].min(axis=1)\n",
    "        \n",
    "# Additional observations\n",
    "add_features = ['cheques_per_child', 'sales_per_child']\n",
    "data['cheques_per_child'] = data['cheque_count_12m_sum'] / (data['children'].fillna(0) + 1)\n",
    "data['sales_per_child'] = data['sale_sum_12m_sum' ] / (data['children'].fillna(0) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По собранным признакам посмотрим на значения-выбросы, которые опредялетются по разным пропорциям (whis) относительно IQR. Полученные записи разделим на записи из тестовой и контрольной групп и их разделим по участию в акции. Посмотрим на пропорции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cycle by chosen features\n",
    "for key in list(collect_features.keys()) + add_features:\n",
    "    print('Column', key)\n",
    "    \n",
    "    # Table format\n",
    "    dataframe = {'Type' : [i+j for i in ['w/o outliers', 'outliers'] for j in ['_test_0', '_test_1', '_control_0', '_control_1']]}\n",
    "    for whis in [1, 1.5, 2, 2.5]:\n",
    "        IQR = data[key].quantile(0.75) - data[key].quantile(0.25)\n",
    "        \n",
    "        # Separate data w/o outliers\n",
    "        sample = data[data[key] <= data[key].quantile(0.75) + IQR * whis]\n",
    "        \n",
    "        # Separate groups\n",
    "        sample_test = sample[sample['group'] == 'test']['response_att'].value_counts()\n",
    "        sample_control = sample[sample['group'] == 'control']['response_att'].value_counts()\n",
    "        \n",
    "        dataframe['whis{}'.format(whis)] = [sample_test[0], sample_test[1], sample_control[0], sample_control[1]]\n",
    "        \n",
    "         # Separate data outliers\n",
    "        sample = data[data[key] > data[key].quantile(0.75) + IQR * whis]\n",
    "        \n",
    "        # Separate groups\n",
    "        sample_test = sample[sample['group'] == 'test']['response_att'].value_counts()\n",
    "        sample_control = sample[sample['group'] == 'control']['response_att'].value_counts()\n",
    "        \n",
    "        dataframe['whis{}'.format(whis)] += [sample_test[0], sample_test[1], sample_control[0], sample_control[1]]\n",
    "    \n",
    "    print(pd.DataFrame(dataframe))\n",
    "    sns.boxplot(data[key])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(data[[key for key in list(collect_features.keys()) + add_features]].corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка матрицы корреляции не выявила какой-то особенной связи.\n",
    "\n",
    "Ни один из проверенных параметров не дал полезного результата: разделяя по выбросам, мы обнаружили, что соотношение людей в тестовой и контрольной группе примерно одно и то же. Отделим выбросы и попытаемся (по whis=1.5) и продолжим поиски. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation parameter\n",
    "whis = 1.5\n",
    "\n",
    "data_map = np.array([])\n",
    "# Cycle by chosen features\n",
    "for key in list(collect_features.keys()) + add_features:\n",
    "    \n",
    "    # Drop outliers\n",
    "    IQR = data[key].quantile(0.75) - data[key].quantile(0.25)\n",
    "    \n",
    "    if not data_map.size:\n",
    "        data_map = (data[key] <= data[key].quantile(0.75) + IQR * whis).values\n",
    "    else:\n",
    "        data_map = data_map & (data[key] <= data[key].quantile(0.75) + IQR * whis).values\n",
    "        \n",
    "data = data[data_map]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Чистка данных\n",
    "Решим проблему с NaN. Сначала посмотрим, сколько строк вообще не имеют ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     532384\n",
       "False       393\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().any(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таких строк очень мало, поэтому придётся как-то заполнять NaN. Если заполнять средними, то это слишком разрежено, поэтому будем заполнять для каждого индекса по-своему. Индекс - это определенные значения колон из \"index_columns\". Проблема в том, что и них встречаются NaN значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чаще всего у покупателя 0.0 детей\n"
     ]
    }
   ],
   "source": [
    "print(\"Чаще всего у покупателя\",\n",
    "      data['children'].value_counts().sort_values(ascending=False).index[0],\n",
    "      \"детей\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_columns = ['gender', 'main_format', 'children']\n",
    "\n",
    "# Let's fill NaN in \"index_columns\"\n",
    "data['group'].replace({'test' : 1, 'control' : 0}, inplace=True)\n",
    "data['gender'].replace({'М' : 2, 'Ж' : 1, 'Не определен' : 0, None : 0}, inplace=True)\n",
    "data['children'].replace({None : -1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>nans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>k_var_sku_price_15d_g49</td>\n",
       "      <td>0.786162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>k_var_disc_share_15d_g49</td>\n",
       "      <td>0.786042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>k_var_count_per_cheque_15d_g34</td>\n",
       "      <td>0.736070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>k_var_sku_price_15d_g34</td>\n",
       "      <td>0.736070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>k_var_disc_share_15d_g34</td>\n",
       "      <td>0.735981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               name      nans\n",
       "120         k_var_sku_price_15d_g49  0.786162\n",
       "85         k_var_disc_share_15d_g49  0.786042\n",
       "62   k_var_count_per_cheque_15d_g34  0.736070\n",
       "119         k_var_sku_price_15d_g34  0.736070\n",
       "84         k_var_disc_share_15d_g34  0.735981"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate percent of records with NaNs\n",
    "def proportion(series):\n",
    "    if (series.loc[False] == series.sum()):\n",
    "        return 0\n",
    "    return series.loc[True] / series.sum()\n",
    "\n",
    "dataframe = {'name' : [], 'nans' : []}\n",
    "for column in data.columns:\n",
    "    dataframe['name'] += [column]\n",
    "    dataframe['nans'] += [proportion(data[column].isna().value_counts())]\n",
    "\n",
    "# Show top 5 by calculated percent\n",
    "pd.DataFrame(dataframe).sort_values(ascending=False, by='nans')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Group mean values\n",
    "group_means = data.groupby(index_columns).mean()\n",
    "group_means.fillna(group_means.mean(), inplace=True)\n",
    "\n",
    "for i in group_means.index:\n",
    "        data.loc[(data['gender'] == i[0]).values &\n",
    "                 (data['main_format'] == i[1]).values &\n",
    "                 (data['children'] == i[2]).values] = data.loc[(data['gender'] == i[0]).values &\n",
    "                     (data['main_format'] == i[1]).values &\n",
    "                     (data['children'] == i[2]).values].fillna(group_means.loc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также оптимизируем данные для скорости просчетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention : takes some time\n",
    "for column in data.columns:\n",
    "    if data[column].dtype == 'float64':\n",
    "        data[column] = data[column].astype('float32')\n",
    "    if data[column].dtype == 'int64':\n",
    "        data[column] = data[column].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(532777, 195)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[original_columns]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Поиск класса людей, которым требуется коммуникация\n",
    "Общий ход действий такой: создадим побольше интерпретируемых признаков и сделаем вектор значений для каждого человека. После этого найдём всем людям из контрольной группы близжайшего по некоторой метрике партнера. Тогда для каждого человека будем знать его поведение как 1 из 4 классов: \\\n",
    "1) идет на акцию, только если с ним коммуницировать \\\n",
    "2) никогда не ходит на акцию \\\n",
    "3) всегда идет на акцию \\\n",
    "4) идет на акцию, только если с ним не коммуницировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.join(pd.get_dummies(data['age']).astype('int32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sale_sum_6m_sum', 'sale_count_6m_sum') не может быть создана: деление на 0\n",
      "('sale_sum_12m_sum', 'sale_count_12m_sum') не может быть создана: деление на 0\n",
      "('sale_sum_3m_sum', 'sale_count_3m_sum') не может быть создана: деление на 0\n"
     ]
    }
   ],
   "source": [
    "for sum_feature in ['cheque_count_12m_g{}', 'cheque_count_3m_g{}',\n",
    "                   'cheque_count_6m_g{}', 'sale_count_12m_g{}',\n",
    "                    'sale_count_6m_g{}', 'sale_count_3m_g{}',\n",
    "                    'sale_sum_6m_g{}', 'sale_sum_3m_g{}',\n",
    "                    'sale_sum_12m_g{}'\n",
    "                   ]:\n",
    "    data.loc[:, sum_feature[:-4]+'_sum'] = data[get_columns_list(data, [sum_feature.format(i) for i in all_groups])].sum(axis=1)\n",
    "    \n",
    "    \n",
    "for mean_feature in ['k_var_count_per_cheque_1m_g{}',\n",
    "                     'k_var_count_per_cheque_3m_g{}',\n",
    "                     'k_var_count_per_cheque_6m_g{}',\n",
    "                     'k_var_disc_share_1m_g{}',\n",
    "                     'k_var_disc_share_3m_g{}',\n",
    "                     'k_var_disc_share_6m_g{}',\n",
    "                     'k_var_sku_price_3m_g{}',\n",
    "                     'k_var_sku_price_6m_g{}',\n",
    "                     'k_var_sku_price_1m_g{}',\n",
    "                   ]:\n",
    "    data.loc[:, sum_feature[:-4]+'_mean'] = data[get_columns_list(data, [mean_feature.format(i) for i in all_groups])].mean(axis=1)\n",
    "    \n",
    "    \n",
    "for over_feature in [('sale_sum_6m_sum', 'sale_count_6m_sum'),\n",
    "                     ('sale_sum_12m_sum', 'sale_count_12m_sum'),\n",
    "                     ('sale_sum_3m_sum', 'sale_count_3m_sum'),\n",
    "                     \n",
    "                    ]:\n",
    "    if (data[over_feature[1]] == 0).any():\n",
    "        print(over_feature, \"не может быть создана: деление на 0\")\n",
    "        continue\n",
    "        \n",
    "    data[over_feature[0]+'/'+over_feature[1]] = \\\n",
    "        data[over_feature[0]] / data[over_feature[1]]\n",
    "    \n",
    "    \n",
    "for prod_feature in [('mean_discount_depth_15d', 'sale_count_3m_sum'),\n",
    "                     ('promo_share_15d', 'sale_count_3m_sum'),\n",
    "                     ('k_var_sku_per_cheque_15d', 'promo_share_15d'), \n",
    "                    ]:\n",
    "    \n",
    "    data[over_feature[0]+'*'+over_feature[1]] = \\\n",
    "        data[over_feature[0]] * data[over_feature[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучаем модель на тестовой группе:\n",
      "[0]\teval_0-ndcg:0.81754\teval_0-map:0.19374\n",
      "[1]\teval_0-ndcg:0.818537\teval_0-map:0.194802\n",
      "[2]\teval_0-ndcg:0.819252\teval_0-map:0.198567\n",
      "[3]\teval_0-ndcg:0.819357\teval_0-map:0.199929\n",
      "[4]\teval_0-ndcg:0.819432\teval_0-map:0.197785\n",
      "\n",
      "Обучаем модель на контрольной группе:\n",
      "[0]\teval_0-ndcg:0.79717\teval_0-map:0.187121\n",
      "[1]\teval_0-ndcg:0.797246\teval_0-map:0.188438\n",
      "[2]\teval_0-ndcg:0.797366\teval_0-map:0.186406\n",
      "[3]\teval_0-ndcg:0.798136\teval_0-map:0.190145\n",
      "[4]\teval_0-ndcg:0.798238\teval_0-map:0.189018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 1.0854891877144335, 'test': 0.707269002855998, 'train_random': 0.5390685745994582, 'test_random': 0.5726376601059114}\n"
     ]
    }
   ],
   "source": [
    "data_input = data.reset_index().drop('CardHolder', axis=1)\n",
    "data_input.loc[:, 'group'].replace({1 : 'test', 0 : 'control'}, inplace=True)\n",
    "\n",
    "params = {'n_estimators': 5, 'eval_metric': ['ndcg', 'map'], 'verbose': True}\n",
    "\n",
    "model = StupidModel(params)\n",
    "score, model = validate_on_holdout(data_input, model)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на участников обеих подгрупп с точки зрения их признаков \\\n",
    "1. Сумма продаж товаров из доступных групп за 3 месяца\n",
    "2. Отклик на предыдущие sms-рассылки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from scipy.linalg import svd\n",
    "\n",
    "sample = data[data['group'] == 1]\n",
    "sample = sample[sample['response_att']==1].fillna(0).drop('response_att', axis=1)\n",
    "sample = (sample - data.drop('response_att', axis=1).mean())\n",
    "U, s, Vh = svd(sample, full_matrices=False)\n",
    "feature1 = (U @ np.diag(s))[:, 0]\n",
    "feature2 = (U @ np.diag(s))[:, 1]\n",
    "sns.scatterplot(feature1, feature2)\n",
    "\n",
    "sample = data[data['group'] == 1]\n",
    "sample = sample[sample['response_att']==0].fillna(0).drop('response_att', axis=1)\n",
    "sample = (sample - data.drop('response_att', axis=1).mean())\n",
    "U, s, Vh = svd(sample, full_matrices=False)\n",
    "feature1 = (U @ np.diag(s))[:, 0]\n",
    "feature2 = (U @ np.diag(s))[:, 1]\n",
    "sns.scatterplot(feature1, feature2)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_default",
   "language": "python",
   "name": "conda-env-py37_default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
